{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05.Language_Model_solution.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jR26RFkwXtvi"},"source":["# Language Model\n","1. DataLoader\n","2. Model\n","3. Trainer\n","4. Generation\n","\n","이번 실습에서는 RNN기반의 Language Model를 구현해서 텍스트를 직접 생성해보는 실습을 진행해보겠습니다.\n","\n","- dataset: WikiText2 (https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2)\n","- model: LSTM\n"]},{"cell_type":"markdown","metadata":{"id":"crVJ36mMlaXP"},"source":["\n","\n","## Import packages"]},{"cell_type":"markdown","metadata":{"id":"zpvlE_XOWS33"},"source":["런타임의 유형을 변경해줍니다.\n","\n","상단 메뉴에서 [런타임]->[런타임유형변경]->[하드웨어가속기]->[GPU]\n","\n","변경 이후 아래의 cell을 실행 시켰을 때, torch.cuda.is_avialable()이 True가 나와야 합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"cqVdEuPQzMAH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661335569031,"user_tz":-540,"elapsed":2658,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"404dfe9f-947f-4687-dbf6-35d9a60a0c69"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torch.optim as optim\n","print(torch.__version__)\n","print(torch.cuda.is_available())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.1+cu113\n","True\n"]}]},{"cell_type":"code","metadata":{"id":"2o3-HPdHLZma"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy as sp\n","import tqdm\n","import os\n","import random\n","import time\n","import datetime\n","\n","# for reproducibility\n","random.seed(1234)\n","np.random.seed(1234)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T1GnKJCB4T_Q"},"source":["# 1.DataLoader\n","\n","이전의 실습들에서 사용한것과 마찬가지로, PyTorch style의 dataloader를 먼저 만들어 두겠습니다."]},{"cell_type":"markdown","metadata":{"id":"wcNl0aWbS0OA"},"source":["### 1-1.Dataset\n","\n","저희가 이번 실습에서 사용할 데이터셋은 Wikipedia에 있는 영문 글들을 가져온 WikiTree dataset입니다.\n","저희가 불러올 데이터는 가장 작은 WikiTree dataset에서 자주 사용되지 않는 단어나 영어가 아닌 단어들은 unknown token ([unk]) 으로 이미 전처리가 되어있습니다."]},{"cell_type":"code","metadata":{"id":"CKf8zNuISiC2"},"source":["import urllib\n","with urllib.request.urlopen('https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/02-intermediate/language_model/data/train.txt') as f:\n","    data = f.readlines()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBLNOlRKSpOI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661335570662,"user_tz":-540,"elapsed":19,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"aceba8f5-d348-4ed1-93f0-266add5817d4"},"source":["print('num_sentence:',len(data))\n","data[100]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num_sentence: 42068\n"]},{"output_type":"execute_result","data":{"text/plain":["b\" plans that give advertisers discounts for maintaining or increasing ad spending have become permanent <unk> at the news <unk> and underscore the fierce competition between newsweek time warner inc. 's time magazine and <unk> b. <unk> 's u.s. news & world report \\n\""]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["data[100].split()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYouCxF8dP19","executionInfo":{"status":"ok","timestamp":1661335570663,"user_tz":-540,"elapsed":15,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"2d002010-14b1-47af-f790-52e4cfc9b61d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[b'plans',\n"," b'that',\n"," b'give',\n"," b'advertisers',\n"," b'discounts',\n"," b'for',\n"," b'maintaining',\n"," b'or',\n"," b'increasing',\n"," b'ad',\n"," b'spending',\n"," b'have',\n"," b'become',\n"," b'permanent',\n"," b'<unk>',\n"," b'at',\n"," b'the',\n"," b'news',\n"," b'<unk>',\n"," b'and',\n"," b'underscore',\n"," b'the',\n"," b'fierce',\n"," b'competition',\n"," b'between',\n"," b'newsweek',\n"," b'time',\n"," b'warner',\n"," b'inc.',\n"," b\"'s\",\n"," b'time',\n"," b'magazine',\n"," b'and',\n"," b'<unk>',\n"," b'b.',\n"," b'<unk>',\n"," b\"'s\",\n"," b'u.s.',\n"," b'news',\n"," b'&',\n"," b'world',\n"," b'report']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# \"나는 밥을 먹는다.\"\n","kor_data = \"나는 밥을 먹는다.\"\n","kor_data.split()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rRQUPLbpdbwU","executionInfo":{"status":"ok","timestamp":1661335570663,"user_tz":-540,"elapsed":11,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"3547ccd7-d1a7-4617-aa01-e13d374fd27a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['나는', '밥을', '먹는다.']"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"RWBv1J5XdbNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OfLTv1EPbSwj","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1661335571213,"user_tz":-540,"elapsed":556,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"7ef9bd94-d797-4d7d-b18a-7b6b1d6bc1d5"},"source":["seq_length_list = []\n","for line in data:\n","    seq_length_list.append(len(line.split()))\n","\n","counts, bins = np.histogram(seq_length_list, bins=20)\n","plt.hist(bins[:-1], bins, weights=counts)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS5klEQVR4nO3dYaxc5X3n8e+vkKQtrbAJXou1rTWrWInoaiGsBY4SVSlsjYEq5kUaEVUbK7LkN95usqrUml1pUZJGItKqlEhbJCu4daIshNJksUgU6nWIVq0U4FIIARzWt8TUtgDfxEC2i5ot6X9fzHOTCbmXe6/v9czYz/cjjeac5zxn5n9mxr9z7jNnjlNVSJL68AvjLkCSNDqGviR1xNCXpI4Y+pLUEUNfkjpy/rgLeDMXX3xxbdy4cdxlSNJZ5bHHHvt+Va2Za9lEh/7GjRuZmpoadxmSdFZJ8vx8yxzekaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjkz0L3K1NBv3fHVZ6x+97cYVqkTSpPJIX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXEUzYnzHJPu5SkN+ORviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIokI/yaok9yX5bpLDSd6T5KIkB5McaferW98k+WyS6SRPJrly6HF2tP5Hkuw4UxslSZrbYo/07wC+XlXvAi4HDgN7gENVtQk41OYBrgc2tdsu4E6AJBcBtwJXA1cBt87uKCRJo7Fg6Ce5EPh14C6Aqvp/VfUKsB3Y37rtB25q09uBz9fAt4BVSS4BrgMOVtWpqnoZOAhsW9GtkSS9qcUc6V8KzAB/muTxJJ9LcgGwtqpeaH1eBNa26XXAsaH1j7e2+dp/RpJdSaaSTM3MzCxtayRJb2oxl2E4H7gS+N2qejjJHfx0KAeAqqoktRIFVdVeYC/A5s2bV+QxtTjLuQSE/+uWdHZYzJH+ceB4VT3c5u9jsBN4qQ3b0O5PtuUngA1D669vbfO1S5JGZMHQr6oXgWNJ3tmargWeAQ4As2fg7ADub9MHgI+0s3i2AK+2YaAHga1JVrcvcLe2NknSiCz2Kpu/C3wxyVuB54CPMthh3JtkJ/A88KHW92vADcA08FrrS1WdSvIp4NHW75NVdWpFtkKStCiLCv2qegLYPMeia+foW8DueR5nH7BvKQVKklaOv8iVpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjiz2v0vUEmzc89VxlyBJc/JIX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4sK/SRHk3wnyRNJplrbRUkOJjnS7le39iT5bJLpJE8muXLocXa0/keS7DgzmyRJms9SjvR/o6quqKrNbX4PcKiqNgGH2jzA9cCmdtsF3AmDnQRwK3A1cBVw6+yOQpI0GssZ3tkO7G/T+4Gbhto/XwPfAlYluQS4DjhYVaeq6mXgILBtGc8vSVqixYZ+AX+Z5LEku1rb2qp6oU2/CKxt0+uAY0PrHm9t87X/jCS7kkwlmZqZmVlkeZKkxVjsL3LfV1Unkvwz4GCS7w4vrKpKUitRUFXtBfYCbN68eUUeU5I0sKgj/ao60e5PAl9hMCb/Uhu2od2fbN1PABuGVl/f2uZrlySNyIKhn+SCJL86Ow1sBZ4CDgCzZ+DsAO5v0weAj7SzeLYAr7ZhoAeBrUlWty9wt7Y2SdKILGZ4Zy3wlSSz/f97VX09yaPAvUl2As8DH2r9vwbcAEwDrwEfBaiqU0k+BTza+n2yqk6t2JZorJZzkbmjt924gpVIejMLhn5VPQdcPkf7D4Br52gvYPc8j7UP2Lf0MiVJK8Ff5EpSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4sOvSTnJfk8SQPtPlLkzycZDrJl5K8tbW/rc1Pt+Ubhx7jltb+bJLrVnpjJElvbilH+h8DDg/Nfwa4vareAbwM7GztO4GXW/vtrR9JLgNuBn4N2Ab8SZLzlle+JGkpFhX6SdYDNwKfa/MBrgHua132Aze16e1tnrb82tZ/O3BPVf2oqr4HTANXrcRGSJIWZ7FH+n8M/D7wT23+7cArVfV6mz8OrGvT64BjAG35q63/T9rnWEeSNAILhn6S3wJOVtVjI6iHJLuSTCWZmpmZGcVTSlI3FnOk/17gA0mOAvcwGNa5A1iV5PzWZz1wok2fADYAtOUXAj8Ybp9jnZ+oqr1VtbmqNq9Zs2bJGyRJmt+CoV9Vt1TV+qrayOCL2G9U1e8ADwEfbN12APe36QNtnrb8G1VVrf3mdnbPpcAm4JEV2xJJ0oLOX7jLvP4AuCfJHwKPA3e19ruALySZBk4x2FFQVU8nuRd4Bngd2F1VP17G80uSlmhJoV9V3wS+2aafY46zb6rqH4Dfnmf9TwOfXmqRkqSV4S9yJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHlnM9fWlFbNzz1dNe9+htN65gJdK5zyN9SeqIoS9JHXF4Zx7LGXKQpEnlkb4kdcTQl6SOGPqS1BFDX5I6YuhLUkcWDP0kv5jkkSTfTvJ0kk+09kuTPJxkOsmXkry1tb+tzU+35RuHHuuW1v5skuvO1EZJkua2mCP9HwHXVNXlwBXAtiRbgM8At1fVO4CXgZ2t/07g5dZ+e+tHksuAm4FfA7YBf5LkvJXcGEnSm1sw9Gvg79vsW9qtgGuA+1r7fuCmNr29zdOWX5skrf2eqvpRVX0PmAauWpGtkCQtyqLG9JOcl+QJ4CRwEPhb4JWqer11OQ6sa9PrgGMAbfmrwNuH2+dYZ/i5diWZSjI1MzOz9C2SJM1rUaFfVT+uqiuA9QyOzt91pgqqqr1VtbmqNq9Zs+ZMPY0kdWlJZ+9U1SvAQ8B7gFVJZi/jsB440aZPABsA2vILgR8Mt8+xjiRpBBZz9s6aJKva9C8BvwkcZhD+H2zddgD3t+kDbZ62/BtVVa395nZ2z6XAJuCRldoQSdLCFnPBtUuA/e1Mm18A7q2qB5I8A9yT5A+Bx4G7Wv+7gC8kmQZOMThjh6p6Osm9wDPA68Duqvrxym6OJOnNLBj6VfUk8O452p9jjrNvquofgN+e57E+DXx66WVKklaCv8iVpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdWTD0k2xI8lCSZ5I8neRjrf2iJAeTHGn3q1t7knw2yXSSJ5NcOfRYO1r/I0l2nLnNkiTNZTFH+q8Dv1dVlwFbgN1JLgP2AIeqahNwqM0DXA9sarddwJ0w2EkAtwJXA1cBt87uKCRJo3H+Qh2q6gXghTb9f5IcBtYB24H3t277gW8Cf9DaP19VBXwryaokl7S+B6vqFECSg8A24O4V3B51ZuOery5r/aO33bhClUhnhyWN6SfZCLwbeBhY23YIAC8Ca9v0OuDY0GrHW9t87ZKkEVl06Cf5FeAvgI9X1Q+Hl7Wj+lqJgpLsSjKVZGpmZmYlHlKS1Cwq9JO8hUHgf7GqvtyaX2rDNrT7k639BLBhaPX1rW2+9p9RVXuranNVbV6zZs1StkWStIDFnL0T4C7gcFX90dCiA8DsGTg7gPuH2j/SzuLZArzahoEeBLYmWd2+wN3a2iRJI7LgF7nAe4F/B3wnyROt7T8BtwH3JtkJPA98qC37GnADMA28BnwUoKpOJfkU8Gjr98nZL3UlSaOxmLN3/grIPIuvnaN/Abvneax9wL6lFChJWjn+IleSOmLoS1JHDH1J6oihL0kdMfQlqSOLOWXzrLXc67JI0rnGI31J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdeScvuCatJDlXJTv6G03rmAl0mh4pC9JHTH0Jakjhr4kdcTQl6SOLBj6SfYlOZnkqaG2i5IcTHKk3a9u7Uny2STTSZ5McuXQOjta/yNJdpyZzZEkvZnFHOn/GbDtDW17gENVtQk41OYBrgc2tdsu4E4Y7CSAW4GrgauAW2d3FJKk0Vkw9KvqfwGn3tC8HdjfpvcDNw21f74GvgWsSnIJcB1wsKpOVdXLwEF+fkciSTrDTndMf21VvdCmXwTWtul1wLGhfsdb23ztPyfJriRTSaZmZmZOszxJ0lyW/UVuVRVQK1DL7OPtrarNVbV5zZo1K/WwkiROP/RfasM2tPuTrf0EsGGo3/rWNl+7JGmETjf0DwCzZ+DsAO4fav9IO4tnC/BqGwZ6ENiaZHX7Andra5MkjdCC195JcjfwfuDiJMcZnIVzG3Bvkp3A88CHWvevATcA08BrwEcBqupUkk8Bj7Z+n6yqN345LEk6wxYM/ar68DyLrp2jbwG753mcfcC+JVUnSVpR/iJXkjpi6EtSRwx9SeqIoS9JHfF/zpJOk//rls5GHulLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kd8do70hh43R6Ni0f6ktQRQ1+SOmLoS1JHHNOXzjJ+H6Dl8Ehfkjpi6EtSR0Y+vJNkG3AHcB7wuaq6bdQ1SL1aztAQODx0Lhhp6Cc5D/hvwG8Cx4FHkxyoqmdGWYek0+P3CWe/UR/pXwVMV9VzAEnuAbYDhr50jlvuXxnjcC7uqEYd+uuAY0Pzx4Grhzsk2QXsarN/n+TZJTz+xcD3l1XhypvEmsC6lmISa4LJrGsSa4LTrCufOQOV/NSZfK3+xXwLJu6UzaraC+w9nXWTTFXV5hUuaVkmsSawrqWYxJpgMuuaxJpgMusaV02jPnvnBLBhaH59a5MkjcCoQ/9RYFOSS5O8FbgZODDiGiSpWyMd3qmq15P8e+BBBqds7quqp1fwKU5rWOgMm8SawLqWYhJrgsmsaxJrgsmsayw1parG8bySpDHwF7mS1BFDX5I6ck6EfpJtSZ5NMp1kzxjr2JfkZJKnhtouSnIwyZF2v3rENW1I8lCSZ5I8neRjE1LXLyZ5JMm3W12faO2XJnm4vZdfal/4j1SS85I8nuSBCarpaJLvJHkiyVRrG+t72GpYleS+JN9NcjjJe8ZZV5J3ttdo9vbDJB+fkNfqP7bP+lNJ7m7/Bkb+2TrrQ3/o0g7XA5cBH05y2ZjK+TNg2xva9gCHqmoTcKjNj9LrwO9V1WXAFmB3e33GXdePgGuq6nLgCmBbki3AZ4Dbq+odwMvAzhHXBfAx4PDQ/CTUBPAbVXXF0Lnd434PYXAdra9X1buAyxm8bmOrq6qeba/RFcC/AV4DvjLOmgCSrAP+A7C5qv4VgxNZbmYcn62qOqtvwHuAB4fmbwFuGWM9G4GnhuafBS5p05cAz4759bqfwbWPJqYu4JeBv2Hw6+zvA+fP9d6OqJb1DELhGuABIOOuqT3vUeDiN7SN9T0ELgS+RzshZFLqGqpjK/DXk1ATP70awUUMzpp8ALhuHJ+ts/5In7kv7bBuTLXMZW1VvdCmXwTWjquQJBuBdwMPMwF1tWGUJ4CTwEHgb4FXqur11mUc7+UfA78P/FObf/sE1ARQwF8meaxdqgTG/x5eCswAf9qGwz6X5IIJqGvWzcDdbXqsNVXVCeC/An8HvAC8CjzGGD5b50LonzVqsDsfyzmySX4F+Avg41X1w0moq6p+XIM/w9czuBjfu0Zdw7AkvwWcrKrHxlnHPN5XVVcyGMbcneTXhxeO6T08H7gSuLOq3g38X94wbDKuz1YbG/8A8OdvXDaOmtp3CNsZ7Cj/OXABPz8UPBLnQuhP+qUdXkpyCUC7PznqApK8hUHgf7Gqvjwpdc2qqleAhxj8ebsqyeyPBkf9Xr4X+ECSo8A9DIZ47hhzTcBPjhSpqpMMxqivYvzv4XHgeFU93ObvY7ATGHddMNg5/k1VvdTmx13TvwW+V1UzVfWPwJcZfN5G/tk6F0J/0i/tcADY0aZ3MBhTH5kkAe4CDlfVH01QXWuSrGrTv8Tge4bDDML/g+Ooq6puqar1VbWRwefoG1X1O+OsCSDJBUl+dXaawVj1U4z5PayqF4FjSd7Zmq5lcJn0sdbVfJifDu3A+Gv6O2BLkl9u/yZnX6vRf7bG8QXLGfiS5AbgfzMYE/7PY6zjbgbjdf/I4ChoJ4Mx4UPAEeB/AheNuKb3MfhT9kngiXa7YQLq+tfA462up4D/0tr/JfAIMM3gT/O3jem9fD/wwCTU1J7/2+329OxnfNzvYavhCmCqvY//A1g97roYDJ38ALhwqG0SXqtPAN9tn/cvAG8bx2fLyzBIUkfOheEdSdIiGfqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI/8f6NrZ90AWNTIAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"4SdattmOcRwC"},"source":["데이터에 있는 문장 길이들의 histogram을 볼 때 대부분의 data의 문장 길이가 50에 미치지 못하기 때문에 \\\\\n","model에 집어넣을 최대 문장 길이를 50으로 세팅해두도록 하겠습니다."]},{"cell_type":"code","metadata":{"id":"g7MuFqsKcd4U"},"source":["max_seq_len = 50"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IyMpsyX8TwYy"},"source":["### 1-2.Build Dictionary\n","\n","먼저 text 데이터를 모델에 넣어주기 위해서는 text에 존재하는 단어들을 index로 변환해주어야 합니다.\n","\n","이를 위해서는 단어를 index로 변환해주는 word2idx dictionary와 다시 index를 단어로 변환해주는 idx2word dictionary를 만들어야 합니다.\n"]},{"cell_type":"code","metadata":{"id":"cZmyZhcpTvZz"},"source":["def build_dictionary(data, max_seq_len):\n","    word2idx = {}\n","    idx2word = {}\n","    ## Build Dictionary\n","    word2idx['<pad>'] = 0\n","    word2idx['<unk>'] = 1\n","    idx2word[0] = '<pad>'\n","    idx2word[1] = '<unk>'\n","    idx = 2\n","    for line in data:\n","        words = line.decode('utf-8').split()\n","        words = words[:max_seq_len]        \n","        ### Build Dictionary to convert word to index and index to word\n","        ### YOUR CODE HERE (~ 5 lines)\n","        for word in words:\n","            if word not in word2idx:\n","                word2idx[word] = idx\n","                idx2word[idx] = word\n","                idx += 1\n","\n","    return word2idx, idx2word\n","\n","word2idx, idx2word = build_dictionary(data, max_seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EPfV0OTc4Xdr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661335571214,"user_tz":-540,"elapsed":20,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"66753fb7-71a3-4761-d312-44d91923a4bf"},"source":["if len(word2idx) == len(idx2word) == 10000:\n","    print(\"Test Passed!\")\n","else:\n","    raise AssertionError"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Passed!\n"]}]},{"cell_type":"markdown","metadata":{"id":"me_m8njoXHrv"},"source":["### 1-3.Preprocessing\n","\n","이제 앞서 만든 dictionary를 이용해서 text로된 데이터셋을 index들로 변환시키겠습니다."]},{"cell_type":"code","metadata":{"id":"I6fuARgzXEDU"},"source":["def preprocess(data, word2idx, idx2word, max_seq_len):\n","    tokens = []\n","    for line in data:\n","        words = line.decode('utf-8').split()\n","        words = words[:max_seq_len]\n","        ### Convert dataset with tokens\n","        ### For each line, append <pad> token to match the number of max_seq_len\n","        ### YOUR CODE HERE (~ 4 lines)\n","        words += ['<pad>']*(max_seq_len - len(words))\n","        for word in words:\n","            token = word2idx[word]\n","            tokens.append(token)\n","\n","    return tokens\n","\n","tokens = preprocess(data, word2idx, idx2word, max_seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjyvqMgbZnfP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661335571215,"user_tz":-540,"elapsed":19,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"0399de0b-4f83-4281-b257-df288172515e"},"source":["if len(tokens) == 2103400:\n","    print(\"Test Passed!\")\n","else:\n","    raise AssertionError"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Passed!\n"]}]},{"cell_type":"markdown","metadata":{"id":"jmQxX3BH-SAv"},"source":["이제 전처리된 Token들을 문장 단위의 배열로 변환시켜 두겠습니다."]},{"cell_type":"code","metadata":{"id":"knMvtp23-Jye","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661335571216,"user_tz":-540,"elapsed":18,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"81e3f2dc-9252-4b04-ddf2-0db82981fb44"},"source":["tokens = np.array(tokens).reshape(-1, max_seq_len)\n","print(tokens.shape)\n","tokens[100]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(42068, 50)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([745,  93, 746, 739, 747, 181, 748, 467, 749, 740, 750, 154, 751,\n","       752,   1, 160,  32, 753,   1,  48, 754,  32, 755, 756, 757, 728,\n","       555, 758,  99, 119, 555, 733,  48,   1, 759,   1, 119, 237, 753,\n","       230, 760, 347,   0,   0,   0,   0,   0,   0,   0,   0])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"pceBqmtTZ9g9"},"source":["### 1-4.DataLoader\n","\n","이제 전처리된 dataset을 활용하여 PyTorch style의 dataset과 dataloader를 만들도록 하겠습니다.\n","\n","Token형태의 데이터를 PyTorch 스타일의 dataset으로 만들 때 주의할 점은, 추후 embedding matrix에서 indexing을 해주기 위해서 각 token이 LongTensor 형태로 정의되어야 한다는 점입니다."]},{"cell_type":"code","metadata":{"id":"1hAwhG1K9iBI"},"source":["class LMDataset(torch.utils.data.Dataset):\n","    def __init__(self, tokens):\n","        super(LMDataset, self).__init__()\n","        self.PAD = 0\n","        self.UNK = 1\n","        self.tokens = tokens\n","        self._getitem(2)\n","\n","    def _getitem(self, index):\n","        X = self.tokens[index]\n","        y = np.concatenate((X[1:], [self.PAD]))\n","\n","        X = torch.from_numpy(X).unsqueeze(0).long()\n","        y = torch.from_numpy(y).unsqueeze(0).long()\n","\n","        return X, y\n","\n","    def __getitem__(self, index):\n","        X = self.tokens[index]\n","        y = np.concatenate((X[1:], [self.PAD]))\n","\n","        X = torch.from_numpy(X).long()\n","        y = torch.from_numpy(y).long()\n","\n","        return X, y\n","\n","    def __len__(self):\n","        return len(self.tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BiLNqM6kAda1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661335571217,"user_tz":-540,"elapsed":16,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"06b978ed-d3df-43d0-b73a-d76eb6d08e33"},"source":["batch_size = 64\n","dataset = LMDataset(tokens)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","print(len(dataset))\n","print(len(dataloader))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["42068\n","658\n"]}]},{"cell_type":"markdown","metadata":{"id":"b1nhBnqWxw4a"},"source":["# 2.Model\n","\n","이번 section에서는 Language Modeling을 위한 Recurrent Model을 직접 만들어보도록 하겠습니다.\n","\n","Standard한 Recurrent Neural Network (RNN) model은 vanishing gradient 문제에 취약하기 때문에, 이번 실습에서는 변형된 RNN구조인 LSTM model을 활용하도록 하겠습니다.\n"]},{"cell_type":"markdown","metadata":{"id":"aOoNVt3MDOjl"},"source":["### 2-1.LSTM"]},{"cell_type":"markdown","metadata":{"id":"9lycT_9vwaJN"},"source":["LSTM model의 전체적인 구조와 각 gate의 수식은 아래와 같습니다.\n","\n","![](https://drive.google.com/uc?export=view&id=1n93tpNW55Xl4GxZNcJcbUVRhuNCGH38h)"]},{"cell_type":"markdown","metadata":{"id":"S1h6nfvYwN8n"},"source":["![](https://drive.google.com/uc?export=view&id=1nH9U5iD9cO6OVVTbrx-LjypRvcWzbOCU)\n","\n","LSTM의 자세한 동작방식이 궁금하신 분은 아래의 블로그를 참조해주세요.\n","\n","https://colah.github.io/posts/2015-08-Understanding-LSTMs/"]},{"cell_type":"code","metadata":{"id":"YDNAysVqxxOk"},"source":["class LSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(LSTMCell, self).__init__()\n","        # input-gate\n","        self.Wi = nn.Linear(input_size + hidden_size, hidden_size)\n","        # forget-gate\n","        self.Wf = nn.Linear(input_size + hidden_size, hidden_size)\n","        # gate-gate\n","        self.Wg = nn.Linear(input_size + hidden_size, hidden_size)\n","        # output-gate\n","        self.Wo = nn.Linear(input_size + hidden_size, hidden_size)\n","\n","        # non-linearity\n","        self.sigmoid = nn.Sigmoid()\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x, h_0, c_0):\n","        \"\"\"\n","        Inputs\n","            input (x): [batch_size, input_size]\n","            hidden_state (h_0): [batch_size, hidden_size]\n","            cell_state (c_0): [batch_size, hidden_size]\n","        Outputs\n","            next_hidden_state (h_1): [batch_size, hidden_size]\n","            next_cell_state (c_1): [batch_size, hidden_size]    \n","        \"\"\"\n","        h_1, c_1 = None, None\n","        input = torch.cat((x, h_0), 1)\n","        # Implement LSTM cell as noted above\n","        ### YOUR CODE HERE (~ 6 lines)\n","        i = self.sigmoid(self.Wi(input))\n","        f = self.sigmoid(self.Wf(input))\n","        g = self.tanh(self.Wg(input))\n","        o = self.sigmoid(self.Wo(input))\n","        c_1 = f * c_0 + i * g\n","        h_1 = o * self.tanh(c_1)\n","\n","        return h_1, c_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0Tff2VCJ56D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661335571217,"user_tz":-540,"elapsed":14,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"88938f2a-c033-454d-89d9-cea247385add"},"source":["def test_lstm():\n","    batch_size = 2\n","    input_size = 5\n","    hidden_size = 3\n","\n","    #torch.manual_seed(1234)\n","    lstm = LSTMCell(input_size ,hidden_size)\n","    def init_weights(m):\n","        if isinstance(m, nn.Linear):\n","            torch.nn.init.constant_(m.weight, 0.1)\n","            m.bias.data.fill_(0.01)\n","    lstm.apply(init_weights)\n","\n","    x = torch.ones(batch_size, input_size)\n","    hx = torch.zeros(batch_size, hidden_size)\n","    cx = torch.zeros(batch_size, hidden_size)\n","\n","    hx, cx = lstm(x, hx, cx)\n","    assert hx.detach().allclose(torch.tensor([[0.1784, 0.1784, 0.1784], \n","                                              [0.1784, 0.1784, 0.1784]]), atol=2e-1), \\\n","            f\"Output of the hidden state does not match.\"\n","    assert cx.detach().allclose(torch.tensor([[0.2936, 0.2936, 0.2936], \n","                                              [0.2936, 0.2936, 0.2936]]), atol=2e-1), \\\n","            f\"Output of the cell state does not match.\"\n","\n","    print(\"==LSTM cell test passed!==\")\n","\n","test_lstm()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==LSTM cell test passed!==\n"]}]},{"cell_type":"markdown","metadata":{"id":"0DxU-78B33dG"},"source":["## 2-2.Language Model\n","\n","이제, 위에서 정의한 LSTM Cell을 활용해서 아래와 같은 Langauge Model을 만들어보도록 하겠습니다.\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1nMAbL-g31nERM44dgohA3k9Vj_92hIh-)"]},{"cell_type":"code","metadata":{"id":"l0U2s0hux_n6"},"source":["class LanguageModel(nn.Module):\n","    def __init__(self, input_size=64, hidden_size=64, vocab_size=10000):\n","        super(LanguageModel, self).__init__()\n","        \n","        self.input_layer = nn.Embedding(vocab_size, input_size)\n","        self.hidden_layer = LSTMCell(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, vocab_size)\n","\n","\n","    def forward(self, x, hx, cx, predict=False):\n","        \"\"\"\n","        Inputs\n","            input (x): [batch_size]\n","            hidden_state (h_0): [batch_size, hidden_size]\n","            cell_state (c_0): [batch_size, hidden_size]\n","            predict: whether to predict and sample the next word\n","        Outputs\n","            output (ox): [batch_size, hidden_size]\n","            next_hidden_state (h_1): [batch_size, hidden_size]\n","            next_cell_state (c_1): [batch_size, hidden_size]    \n","        \"\"\"\n","        x = self.input_layer(x)\n","        hx, cx = self.hidden_layer(x, hx, cx)\n","        ox = self.output_layer(hx)\n","\n","        if predict == True:\n","            probs = F.softmax(ox, dim=1)\n","            # torch distribution allows sampling operation\n","            # see https://pytorch.org/docs/stable/distributions.html\n","            dist = torch.distributions.Categorical(probs)\n","            ox = dist.sample()\n","\n","        return ox, hx, cx  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G-ZpuMhsbBS8"},"source":["# 3.Trainer\n","\n","자 이제 위에서 구현한 dataloader와 langauge model을 활용해서 모델의 학습을 진행해보도록 하겠습니다.\n"]},{"cell_type":"code","metadata":{"id":"y7TY7HmvbRlB"},"source":["class Trainer():\n","    def __init__(self, \n","                 word2idx, \n","                 idx2word,\n","                 dataloader, \n","                 model, \n","                 criterion,\n","                 optimizer, \n","                 device):\n","        \"\"\"\n","        dataloader: dataloader\n","        model: langauge model\n","        criterion: loss function to evaluate the model (e.g., BCE Loss)\n","        optimizer: optimizer for model\n","        \"\"\"\n","        self.word2idx = word2idx\n","        self.idx2word = idx2word\n","        self.dataloader = dataloader\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.device = device\n","        \n","    def train(self, epochs = 1):\n","        self.model.to(self.device)\n","        start_time = time.time()\n","        for epoch in range(epochs):\n","            losses = []\n","            for iter, (x_batch, y_batch) in tqdm.tqdm(enumerate(self.dataloader)):\n","                self.model.train()\n","                \n","                batch_size, max_seq_len = x_batch.shape\n","                x_batch = x_batch.to(self.device)\n","                y_batch = y_batch.to(self.device)\n","\n","                # initial hidden-states\n","                hx = torch.zeros(batch_size, hidden_size).to(self.device)\n","                cx = torch.zeros(batch_size, hidden_size).to(self.device)\n","\n","                # Implement LSTM operation\n","                ox_batch = []\n","                # Get output logits for each time sequence and append to the list, ox_batch\n","                # YOUR CODE HERE (~ 4 lines)\n","                for s_idx in range(max_seq_len):\n","                    x = x_batch[:, s_idx]\n","                    ox, hx, cx = self.model(x, hx, cx)\n","                    ox_batch.append(ox)\n","                # outputs are ordered by the time sequence\n","                ox_batch = torch.cat(ox_batch).reshape(max_seq_len, batch_size, -1)\n","                ox_batch = ox_batch.permute(1,0,2).reshape(batch_size*max_seq_len, -1)\n","                y_batch = y_batch.reshape(-1)\n","\n","                self.model.zero_grad()\n","                loss = self.criterion(ox_batch, y_batch)\n","                loss.backward()\n","                self.optimizer.step()\n","                losses.append(loss.item())\n","\n","            end_time = time.time() - start_time\n","            end_time = str(datetime.timedelta(seconds=end_time))[:-7]\n","            print('Time [%s], Epoch [%d/%d], loss: %.4f'\n","                  % (end_time, epoch+1, epochs, np.mean(losses)))\n","            if epoch % 5 == 0:\n","                generated_sentences = self.test()\n","                print('[Generated Sentences]')\n","                for sentence in generated_sentences:\n","                    print(sentence)\n","            \n","    def test(self):\n","        # Test model to genereate the sentences\n","        self.model.eval()\n","        num_sentence = 5\n","        max_seq_len = 50\n","\n","        # initial hidden-states\n","        outs = []\n","        x = torch.randint(0, 10000, (num_sentence,)).to(self.device)\n","        hx = torch.zeros(num_sentence, hidden_size).to(self.device)\n","        cx = torch.zeros(num_sentence, hidden_size).to(self.device)\n","\n","        outs.append(x)\n","        with torch.no_grad():\n","            for s_idx in range(max_seq_len-1):\n","                x, hx, cx = self.model(x, hx, cx, predict=True)\n","                outs.append(x)\n","        outs = torch.cat(outs).reshape(max_seq_len, num_sentence)\n","        outs = outs.permute(1, 0)\n","        outs = outs.detach().cpu().numpy()\n","\n","        sentences = []\n","        for out in outs:\n","            sentence = []\n","            for token_idx in out:\n","                word = self.idx2word[token_idx]\n","                sentence.append(word)\n","            sentences.append(sentence)\n","       \n","        return sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgEJv1vWqNkS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661336391806,"user_tz":-540,"elapsed":820601,"user":{"displayName":"박정호","userId":"04165485851555310341"}},"outputId":"9d31229c-613f-44d6-c00e-cc68fa4041d7"},"source":["lr = 1e-2\n","input_size = 128\n","hidden_size = 128\n","batch_size = 256\n","\n","dataset = LMDataset(tokens)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","model = LanguageModel(input_size=input_size, hidden_size=hidden_size)\n","# NOTE: you should use ignore_index to ignore the loss from predicting the <PAD> token\n","criterion = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","device = torch.device('cuda')\n","\n","trainer = Trainer(word2idx = word2idx,\n","                  idx2word = idx2word,\n","                  dataloader=dataloader, \n","                  model = model,\n","                  criterion=criterion,\n","                  optimizer = optimizer,\n","                  device=device)\n","\n","trainer.train(epochs=50)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["165it [00:23,  7.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:00:23], Epoch [1/50], loss: 6.0914\n","[Generated Sentences]\n","['bebear', 'bank', 'services', 'range', 'minutes', 'of', 'native', 'equitable', 'to', 'labor', 'and', '<unk>', '<unk>', 'in', 'the', 'matter', 'of', 'the', 'recent', 'ad', 'expects', 'later', 'the', 'sidelines', 'in', 'the', 'weekend', 'N', 'junk', 'market', 'president', 'of', 'the', 'city', 'committee', 'ought', 'to', 'strange', 'if', 'anything', 'agreeing', 'involved', 'in', 'transfer', 'and', 'the', 'entire', 'u.s.', 'fund', 'facilities']\n","['temperatures', 'tap', 'steven', 'step', 'are', 'officials', 'to', 'them', 'after', 'that', '<unk>', '<unk>', 'a', 'duty', 'could', 'be', 'already', 'caused', 'available', 'in', 'sluggish', '<unk>', 'practices', 'ca', \"n't\", 'act', 'that', 'they', 'do', 'the', 'baby', 'is', 'made', 'to', 'bans', 'and', 'their', 'governments', 'like', 'investors', 'say', 'we', 'can', 'got', 'their', 'legislative', 'idea', 'ruling', 'the', 'rural']\n","['admission', 'together', 'yes', 'between', 'the', '<unk>', 'administration', 'struggling', 'in', 'she', 'wants', 'to', 'total', 'paying', 'tight', 'from', 'management-led', 'and', 'exchange', 'basis', 'remaining', 'N', 'N', 'in', 'those', 'rather', 'than', 'ford', 'motor', 'division', 'had', 'been', 'a', 'loss', 'as', 'well', 'as', 'more', 'than', '$', 'N', 'million', 'preferred', 'but', 'after', 'friday', 'is', 'leading', 'a', 'year']\n","['color', 'journal', 'veto', 'began', 'lynch', 'level', 'as', 'well', 'another', 'june', 'and', 'urban', 'interest', 'rates', 'substantially', 'about', '$', 'N', 'to', '$', 'N', 'billion', 'in', 'the', 'top', 'of', 'the', 'u.s.', 'cars', 'and', 'marketing', 'as', 'friday', 'it', 'lowe', 'a', 'initiative', 'of', 'charity', 'and', 'worse', 'shareholders', 'circuit', 'available', 'out', 'at', 'c$', 'N', 'million', 'and']\n","['aides', 'were', 'headed', 'after', '<unk>', 'in', 'its', 'flight', 'fiber', 'against', 'financial', 'services', 'unions', 'he', 'and', 'expects', 'as', 'data', 'markets', 'and', 'also', 'has', \"n't\", 'back', 'his', 'yearly', 'right', 'higher', 'to', '<unk>', 'to', 'the', 'bank', 'said', 'not', 'going', 'to', 'all', 'products', 'retirement', 'an', 'u.s.', 'posner', 'workers', 'should', 'be', 'delivering', 'its', '<unk>', 'variety']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:15, 10.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:00:39], Epoch [2/50], loss: 5.2233\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:15, 10.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:00:55], Epoch [3/50], loss: 4.9047\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:15, 10.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:01:11], Epoch [4/50], loss: 4.6957\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16,  9.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:01:27], Epoch [5/50], loss: 4.5364\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:01:43], Epoch [6/50], loss: 4.4092\n","[Generated Sentences]\n","['distributed', 'at', 'returning', 'drugs', '<unk>', 'some', 'walls', 'and', 'work', 'after', 'the', 'case', 'will', 'act', 'we', \"'ve\", 'been', 'created', 'in', 'the', 'cost', 'of', 'being', '<unk>', 'to', 'the', 'power', 'round', 'of', 'substantially', 'their', '<unk>', 'conflicts', 'so', 'many', 'lawmakers', 'says', 'sir', 'james', 'a.', 'johnson', 'jr.', 'led', 'by', 'farmers', 'components', 'inc.', 'on', 'its', 'addition']\n","['shouting', 'wind', 'up', 'always', 'remain', 'wary', 'of', 'the', 'federal', 'government', \"'s\", 'request', 'for', 'the', 'high', 'tax', 'show', 'mr.', 'straszheim', \"'s\", 'school', 'who', 'resigned', 'and', 'the', 'policy', 'had', 'had', 'made', 'it', 'to', 'do', 'so', 'soon', 'to', 'be', 'the', 'game', 'cold', 'fusion', 'experiments', 'to', 'provide', 'the', 'horrible', 'certain', 'cancer', 'opportunity', 'to', 'the']\n","['death', 'encouraging', 'customers', 'suspension', 'to', 'add', 'the', 'cross', 'economy', '<unk>', 'he', 'have', 'a', 'line-item', 'veto', 'act', 'has', 'considered', 'the', 'senate', 'staff', 'to', 'get', 'a', 'series', 'of', 'the', 'civil', 'war', 'much', 'of', 'some', '<unk>', 'to', 'seven', 'cases', 'citicorp', 'narrower', 'the', 'air-freight', 'company', 'from', 'the', '<unk>', 'production', 'of', 'service', 'options', 'abuse', 'and']\n","['places', 'it', 'will', 'sell', 'a', 'N', 'N', 'increase', 'in', 'the', 'decline', 'in', 'gold', 'and', 'market', 'prices', 'from', 'the', 'december', 'two', 'months', 'N', 'N', 'season', 'about', 'the', 'takeover', 'boom', 'and', 'a', 'section', 'of', 'future', 'revenue', '<unk>', 'revised', 'N', 'N', 'full', 'annual', 'payments', 'from', 'N', 'million', 'shares', 'issued', 'by', 'asia', 'and', 'its']\n","['fixed-price', 'ourselves', 'attempting', 'that', 'this', 'middle', 'of', 'as', '<unk>', 'effects', 'companies', 'at', 'least', 'such', 'military', 'aid', 'to', 'block', 'boston', 'tried', 'permission', 'from', 'blow', 'to', 'french', 'or', 'the', 'sand', 'again', 'last', 'week', 'attempting', '<unk>', 'he', 'said', 'that', 'he', 'would', 'be', 'offset', 'separately', 'with', 'all', 'countries', 'to', 'the', 'federal', 'debt', 'spending', 'is']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16,  9.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:02:00], Epoch [7/50], loss: 4.3017\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:02:16], Epoch [8/50], loss: 4.2109\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:02:33], Epoch [9/50], loss: 4.1303\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:02:49], Epoch [10/50], loss: 4.0613\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:03:05], Epoch [11/50], loss: 3.9982\n","[Generated Sentences]\n","['mount', 'advancing', 'two', 'years', 'ago', 'the', 'increased', 'problem', 'its', 'lost', 'contact', 'with', 'on', '<unk>', 'small', 'so-called', 'bells', 'intimate', '<unk>', 'ca', 'to', 'make', '<unk>', 'more', 'information', 'around', 'polaroid', 'and', 'information', 'to', 'prosecutors', 'not', 'only', 'the', 'khmer', 'rouge', 'child', 'involved', '<unk>', 'milk', '<unk>', 'and', 'talent', 'of', 'giving', 'research', '<unk>', 'to', 'japan', 'says']\n","['karen', 'mitchell', 'analyst', 'for', 'painewebber', 'group', 'inc.', 'in', 'boston', 'corp.', 'the', 'american', 'defense', '&', 'north', 'american', 'firms', 'the', 'bank', 'switzerland', '&', 'finance', 'group', 'by', 'post', 'that', 'means', 'credits', 'appear', 'commission', 'over', 'part', 'of', 'caci', 'annual', 'sales', '$', 'N', 'million', 'otc', 'on', 'its', 'light', 'price', 'discrepancies', 'between', 'cineplex', \"'s\", 'california', 'plant']\n","['levine', 'have', 'no', 'desire', 'to', 'publish', 'the', 'network', 'they', 'are', 'quietly', 'practicing', 'forms', 'of', 'unconstitutional', 'conditions', 'including', 'fruit', '<unk>', '<unk>', 'mix', 'in', 'such', 'money', 'as', 'often', 'although', 'materials', 'are', 'free-market', 'to', 'no', 'specific', 'good', 'judgment', 'were', 'intended', 'to', 'go', 'into', 'control', 'of', 'bankruptcy', 'trading', 'strategy', 'to', 'the', 'public', '<unk>', 'when']\n","['projected', 'then', 'this', 'year', \"'s\", 'level', 'pace', 'would', 'bounce', 'on', 'sales', 'a', 'bonus', 'for', 'claims', 'against', 'the', 'company', \"'s\", 'restructuring', 'in', 'a', 'filing', 'said', 'edwin', 'l.', '<unk>', 'chief', 'economist', 'at', 'kleinwort', 'benson', 'magazine', 'and', 'no', 'longer', 'hold', 'only', '$', 'N', 'million', 'of', 'its', 'common', 'stock', 'dividend', 'and', 'its', 'position', 'in']\n","['bans', 'loose', '<unk>', 'becomes', 'known', 'as', 'high', 'this', 'bill', 'for', 'state', 'officials', 'including', 'the', '<unk>', 'tramp', 'made', 'in', 'recent', 'years', 'he', 'said', 'the', 'introduction', 'of', 'the', 'two', '<unk>', 'were', 'at', 'a', '<unk>', '<unk>', 'which', 'could', 'force', 'rico', 'securities', 'in', 'coming', 'months', 'through', 'the', 'new', 'drug', 'and', 'several', 'legitimate', 'customers', '<unk>']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:03:21], Epoch [12/50], loss: 3.9407\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:03:37], Epoch [13/50], loss: 3.8910\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:03:53], Epoch [14/50], loss: 3.8451\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:04:10], Epoch [15/50], loss: 3.8019\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:04:26], Epoch [16/50], loss: 3.7647\n","[Generated Sentences]\n","['jr', 'finnish', '<unk>', '<unk>', 'former', 'writer', 'who', 'took', 'a', 'japanese', 'journalist', 'that', 'the', 'show', 'brooks', 'brothers', 'said', 'the', 'company', 'has', 'been', 'both', 'later', 'in', 'any', 'legal', 'commitment', 'from', 'lenders', 'to', 'make', 'sure', 'larger', 'and', 'computer', 'chips', 'on', '<unk>', 'like', 'general', 'electric', 'zones', 'and', 'a', 'luxury', 'infiniti', 'or', '<unk>', 'efforts', 'on']\n","['vincent', '<unk>', '<unk>', 'whose', 'prototype', '<unk>', 'in', 'australia', 'to', 'the', 'emergency', 'producers', 'add', 'fiat', 'research', 'and', 'joseph', 'm.', 'trotter', 'iii', 'and', 'william', '<unk>', 'd.', '<unk>', 'a', 'small', 'investor', 'who', 'reached', 'an', 'investment', 'banking', 'division', 'with', 'individual', 'gov.', 'schools', 'scattered', 'and', 'the', 'two', 'jacobson', 'who', 'spoke', 'on', 'the', 'long-term', 'ad', 'plan']\n","['caution', 'made', 'two', 'work', 'of', 'a', 'specific', 'franchise', 'with', 'the', 'only', 'regulatory', 'problems', 'of', 'recovery', 'could', 'strain', 'phone', 'and', 'imports', 'of', 'the', 'federal', 'deposit', 'insurance', 'corp.', 'a', 'similar', 'coalition', 'briefing', 'as', '<unk>', 'publisher', 'prentice', 'hall', 'and', '<unk>', '<unk>', '<unk>', 'whose', 'new', 'york', 'investment', 'terminal', 'which', 'has', 'an', 'expected', 'acquisition', 'of']\n","['success', 'currently', 'are', 'looking', 'for', 'a', 'carrier', \"'s\", 'alaskan', 'oil', 'company', 'and', 'natural', 'office', 'technological', 'innovation', 'of', 'similar', 'hard-line', 'services', 'to', 'employees', 'to', 'add', 'foreign', 'ownership', 'businessmen', 'and', 'taiwan', 'a', 'cheaper', 'publishing', 'group', 'from', 'a', 'N', 'N', 'stake', 'in', 'eliminating', 'capital', 'and', 'forecasting', 'primarily', 'could', 'sit', 'forward', 'to', 'a', 'consortium']\n","['corners', 'warburg', \"'s\", 'columbus', 'n.j.', 'finance', 'battery', 'park', 'ill.', 'N', 'N', 'of', 'royal', \"'s\", 'own', 'shares', 'which', 'stepped', 'and', 'milk', 'on', 'race', 'and', '<unk>', '<unk>', '<unk>', 'areas', 'had', 'scheduled', 'to', 'reduce', 'ranges', 'from', 'a', 'year', 'ago', 'but', 'the', 'purchase', 'remains', 'nov.', 'N', 'N', 'a', 'month', 'ago', 'according', 'to', 'quebecor', 'through']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:04:42], Epoch [17/50], loss: 3.7286\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:04:58], Epoch [18/50], loss: 3.6966\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:05:14], Epoch [19/50], loss: 3.6645\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:05:30], Epoch [20/50], loss: 3.6383\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:05:46], Epoch [21/50], loss: 3.6131\n","[Generated Sentences]\n","['clearly', 'the', 'powers', 'takes', 'effect', 'even', '<unk>', 'that', 'mr.', 'smith', 'should', 'have', 'turned', 'up', 'to', 'it', 'several', 'trends', 'in', 'the', '<unk>', 'nagging', '<unk>', 'and', 'function', 'need', 'considering', '<unk>', 'to', 'use', 'a', 'debate', 'in', 'the', 'capability', 'to', 'stop', 'and', 'execute', 'departments', 'in', 'the', 'city', 'for', 'red', 'square', 'plans', 'to', 'write', 'a']\n","['begun', 'military', 'nicholas', 'remain', '<unk>', 'by', 'the', 'new', 'york', 'company', 'that', 'excluding', 'the', 'dollar', 'against', 'the', 'warsaw', 'pact', 'with', 'proposed', 's&l', 'of', '<unk>', 'leading', 'electrical', 'engineering', 'and', 'marketing', 'operations', 'at', 'washington', 'jan.', 'N', 'succeeding', 'john', '<unk>', 'an', 'analyst', 'with', 'eli', 'lilly', '&', 'co.', 'in', 'a', 'house', 'speech', 'made', 'by', 'ogden']\n","['trained', '<unk>', 'and', '<unk>', 'forms', 'the', 'chaotic', 'issue', 'is', 'that', 'each', 'of', 'those', 'helpful', 'service', 'is', '<unk>', 'that', 'day', 'be', '<unk>', 'group', 'of', 'new', 'industries', 'inc', 'japan', \"'s\", 'guilty', 'plea', 'to', 'ethics', 'and', 'a', 'plastic', 'wrap', 'suit', 'yesterday', 'called', '<unk>', \"'s\", '<unk>', 'investment', 'bank', 'and', 'finance', 'facilities', 'in', 'staff', \"'\"]\n","['partnership', 'thoughts', 'on', 'its', 'own', 'transaction', 'and', 'its', 'sister', 'service', 'today', 'sponsored', 'for', 'a', 'partner', 'at', 'the', '<unk>', 'model', 'of', 'microprocessors', 'and', '<unk>', 'to', 'display', 'its', 'coast', 'and', 'N', 'residential', '<unk>', 'adding', 'general', \"'s\", 'operations', 'because', 'the', 'finnish', 'shipyard', 'case', 'thomas', 'jefferson', 'did', 'a', '$', 'N', 'fine', 'for', '$', 'N']\n","['voting', 'routinely', 'anyway', 'so', '<unk>', 'has', 'been', 'bought', 'back', 'from', 'beijing', \"'s\", 'new', 'control', 'such', 'as', 'new', 'york', 'and', 'the', 'customer', 'stopped', 'opening', 'up', 'the', 'first', 'two', 'years', 'and', 'landing', 'with', '<unk>', 'disease', 'and', 'soft', 'drinks', 'and', 'the', 'u.s.', 'currency', 'opened', 'in', 'quiet', 'but', '<unk>', 'with', 'a', 'reassuring', 'law', 'requiring']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:06:02], Epoch [22/50], loss: 3.5907\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:06:19], Epoch [23/50], loss: 3.5662\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:06:35], Epoch [24/50], loss: 3.5468\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:06:51], Epoch [25/50], loss: 3.5277\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:07:07], Epoch [26/50], loss: 3.5084\n","[Generated Sentences]\n","['semiconductors', 'mateo', 'happens', 'relief', 'cooperation', 'the', '<unk>', 'brother', '<unk>', 'in', 'his', '<unk>', '<unk>', 'a', 'contemporary', '<unk>', '<unk>', '<unk>', '<unk>', 'and', 'that', '<unk>', 'does', \"n't\", 'increase', 'the', 'sec', 'does', \"n't\", 'work', 'with', 'congressional', 'criticism', 'he', 'has', '<unk>', 'she', 'adds', 'h&r', 'block', 'grant', 'recipients', '<unk>', 'it', 'in', 'the', 'process', 'of', 'winning', 'a']\n","['jerome', 'triple', 'your', '<unk>', 'in', 'any', 'new', 'somehow', 'bought', 'from', 'the', 'provision', 'in', 'the', 'day', \"'s\", 'N', 'hours', 'a', 'year', 'if', 'not', 'mean', 'they', 'had', 'a', 'century', 'at', 'it', 'to', 'get', 'back', 'to', 'square', 'mr.', 'guber', 'and', 'his', 'audience', \"'s\", 'aggressive', 'in', 'the', '<unk>', 'the', 'number', 'of', 'blacks', 'or', 'part']\n","['concern', 'that', 'makes', 'supermarket', '<unk>', 'versions', 'of', '<unk>', 'advertising', 'to', 'drexel', 'burnham', 'lambert', 'inc.', 'whose', 'airlines', 'asked', 'how', 'in', 'los', 'the', 'kentucky', 'commonwealth', '<unk>', 'lens', 'all', 'citizens', 'of', 'p&g', \"'s\", 'and', '<unk>', '<unk>', 'brokerage', 'control', 'that', 'provide', 'up', 'a', 'number', 'of', '<unk>', 'used', 'in', 'the', '<unk>', 'ill.', 'drugs', 'he', 'was']\n","['curbing', 'relatively', 'anyone', 'implicit', 'the', 'hurricane', 'is', 'mixed', 'and', 'that', 'involved', '<unk>', 'at', 'this', 'time', 'he', 'withheld', 'his', 'warner', 'raised', 'his', 'own', 'weight', 'to', 'those', 'words', 'through', 'materials', 'corn', 'seeds', 'are', 'said', 'advertisers', 'has', 'not', 'the', 'dinkins', 'reeling', 'piece', 'on', 'the', 'bill', 'and', 'around', 'setting', 'the', 'stage', 'of', 'inappropriate', 'or']\n","['amicable', 'renew', 'nestle', 'managers', 'georgia-pacific', 'has', 'plenty', 'of', 'ian', 'river', 'living', 'backed', 'by', 'mr.', '<unk>', \"'s\", 'television', 'plants', 'for', '<unk>', 'increases', 'nearly', 'N', 'N', 'of', 'jaguar', 'shares', 'that', 'it', 'has', 'a', 'massive', 'public', 'debt', 'ceiling', 'ever', 'pumped', 'onto', 'their', '<unk>', 'and', 'audio', '<unk>', 'to', 'produce', 'a', 'profit', 'from', 'an', 'earlier']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:07:23], Epoch [27/50], loss: 3.4913\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:07:39], Epoch [28/50], loss: 3.4758\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:07:56], Epoch [29/50], loss: 3.4597\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:08:12], Epoch [30/50], loss: 3.4463\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:08:28], Epoch [31/50], loss: 3.4343\n","[Generated Sentences]\n","['bradstreet', \"'s\", 'junk', 'bond', 'buyer', 'said', 'it', 'has', 'only', 'N', 'million', 'common', 'shares', 'of', 'issues', 'for', 'producing', 'funds', 'available', 'for', 'next', 'year', \"'s\", '$', 'N', 'million', 'mortgage', 'securities', 'but', 'depressed', 'gold', 'reserves', 'improve', 'generally', 'contracting', 'under', 'the', 'company', 'said', 'third-quarter', 'net', 'income', 'rose', 'N', 'N', 'to', '$', 'N', 'million', 'or']\n","['x', 'there', 'are', 'angry', 'ibm', 'for', 'an', 'accident', 'and', 'the', 'idea', 'that', 'they', '<unk>', 'all', 'everything', 'can', 'be', 'printed', 'in', 'leipzig', 'from', 'N', 'he', 'said', 'in', 'an', 'interview', 'oct.', 'N', 'of', 'investors', 'once', 'the', 'issue', 'for', 'foreign', 'magazines', 'oversight', 'and', 'may', 'determine', 'how', 'supreme', 'electoral', 'aid', 'is', 'that', 'it', 'is']\n","['cooking', 'class-action', 'letter', 'of', 'a', 'distinguished', '<unk>', 'the', 'market', 'did', 'not', 'out', 'of', 'his', 'usx', 'struggle', 'successfully', 'challenged', '<unk>', 'he', 'heads', 'N', 'a', 'bonus', 'on', 'the', 'petrochemical', 'company', 'and', 'an', 'annual', 'deductible', 'shopping', 'spree', 'during', 'which', '<unk>', 'customers', 'such', 'as', 'other', '<unk>', 'indicators', 'throughout', 'delaware', 'and', 'a', 'drug', 'group', 'of']\n","['levels', 'tentatively', 'priced', 'down', 'in', 'in', 'active', 'although', 'they', 'were', 'at', 'N', 'down', 'while', 'u.s.', 'sales', 'have', 'increased', 'N', 'N', 'it', 'N', 'metric', 'tons', 'in', 'the', 'gain', 'of', '<unk>', 'from', 'single-family', 'operations', 'rose', 'only', 'N', 'N', 'in', 'august', 'to', 'N', 'N', 'on', 'the', 'week', 'most', 'advanced', 'N', 'N', 'to', 'N']\n","['specialist', 'waterworks', 'could', 'be', 'first', 'principal', 'interest', 'in', 'the', 'service', 'may', 'strike', 'until', 'N', 'p.m.', '<unk>', '<unk>', 'built', 'by', 'the', 'federal', 'government', 'report', 'dropped', 'to', 'certain', 'accounting', 'operations', 'including', 'an', '<unk>', 'office', 'and', 'a', '<unk>', 'concern', 'about', 'N', 'employees', 'at', 'N', 'a.m.', 'to', 'N', 'N', 'before', 'recovering', 'four', 'but', 'a']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:08:44], Epoch [32/50], loss: 3.4218\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:15, 10.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:09:00], Epoch [33/50], loss: 3.4105\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:09:16], Epoch [34/50], loss: 3.4001\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:09:32], Epoch [35/50], loss: 3.3903\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:09:48], Epoch [36/50], loss: 3.3806\n","[Generated Sentences]\n","['battered', 'cure', 'no', 'emergency', 'relief', 'for', 'governors', 'over', 'the', 'past', 'week', 'or', 'too', 'many', 'of', 'these', 'actions', 'and', 'at', 'noon', 'promised', 'to', 'notify', 'federal', 'reserve', 'to', 'promote', 'initiatives', 'show', 'what', 'of', 'both', 'houses', 'were', 'the', 'bid', 'to', 'insurance', 'collect', 'through', 'coffee', 'cases', 'from', 'court', 'competing', '<unk>', 'and', 'public', 'utilities', 'in']\n","['commitments', 'outperformed', 'international', 'business', 'for', 'merchant', 'banking', \"'s\", 'says', 'john', 'lang', 'reservations', 'the', 'u.s.', 'government', 'voted', 'instead', 'of', 'more', '<unk>', 'before', 'they', \"'d\", 'let', 'a', 'satisfaction', 'to', 'the', '<unk>', 'group', 'of', 'acquiring', 'the', 'western', 'desire', 'of', 'the', 'nation', \"'s\", 'largest', 'securities', 'units', 'just', 'so', 'far', 'is', 'this', 'to', 'say', 'they']\n","['va', 'urges', 'greater', 'than', 'do', 'from', 'a', 'renaissance', \"'s\", 'drop', 'in', 'management', '<unk>', 'until', 'now', 'they', \"'re\", '<unk>', 'because', 'the', 'alley', 'theater', 'named', 'an', '<unk>', 'an', 'editor', 'at', '<unk>', 'fla', 'oct.', 'N', 'or', 'winning', 'los', 'angeles', 'citicorp', 'a', 'new', 'york', 'ad', 'agency', 'that', 'just', 'two', 'years', 'ago', 'but', 'he', \"'s\"]\n","['fancy', 'topics', 'like', '<unk>', 'the', '<unk>', 'probably', 'identified', 'for', 'creative', 'products', 'through', 'the', 'state', 'to', 'do', 'the', 'distribution', 'promise', '<unk>', 'so', 'that', 'and', '<unk>', 'her', 'i', 'experienced', 'fighting', 'this', 'to', 'does', 'it', 'grow', 'from', 'resistance', 'to', 'service', 'as', 'then', 'that', 'after', 'a', 'highly', 'life', 'promotions', 'allowing', 'children', 'that', 'the', 'government']\n","['london-based', 'austin', 'texas', 'hub', 'in', 'the', 'fiscal', 'year', 'saw', 'a', 'class-action', '<unk>', 'giving', 'the', 'cost', 'increases', 'under', 'mitsubishi', 'terminal', 'grew', 'N', 'N', 'to', 'N', 'N', 'of', 'their', 'shares', 'at', 'the', 'selling', 'period', 'to', 'N', 'billion', 'yen', '$', 'N', 'billion', 'and', 'suffered', 'small', '<unk>', 'vehicles', 'project', 'china', 'which', 'generally', 'expected', 'the']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:10:05], Epoch [37/50], loss: 3.3746\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:10:21], Epoch [38/50], loss: 3.3645\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:10:37], Epoch [39/50], loss: 3.3569\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:10:53], Epoch [40/50], loss: 3.3485\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:11:09], Epoch [41/50], loss: 3.3393\n","[Generated Sentences]\n","['whether', 'it', 'is', 'a', '<unk>', 'that', 'have', 'been', 'sold', 'for', 'about', '$', 'N', 'of', 'an', 'equity', 'fund', 'products', 'on', 'the', 'common', 'stock', 'fund', 'this', 'year', 'said', 'its', 'plan', 'to', 'bolster', 'net', 'income', 'of', '$', 'N', 'million', 'or', 'N', 'cents', 'a', 'share', 'from', '$', 'N', 'million', 'or', 'N', 'cents', 'a', 'share']\n","['emissions', 'trimmed', 'trim', 'closer', 'access', 'to', 'managers', 'when', 'markets', 'participation', 'in', 'singapore', 'which', 'provides', 'sluggish', 'for', 'the', 'contract', 'directly', 'to', 'provide', 'the', 'problems', 'of', 'technology', 'before', 'the', 'local', 'subway', 'line', 'to', '<unk>', 'near', '<unk>', '<unk>', 'with', 'an', 'import', 'and', 'local', 'views', 'on', 'the', 'corporate', '<unk>', 'and', '<unk>', 'they', \"'d\", 'regulate']\n","['monitoring', 'al', '<unk>', 'for', 'maryland', 'gives', 'the', '<unk>', 'method', 'of', 'the', 'new', 'york', 'times', \"'s\", 'flashy', 'side', 'of', 'school', 'activity', 'i', \"'ll\", 'hope', 'for', 'imbalances', 'with', 'the', 'short-term', 'outlook', 'yield', 'on', 'those', 'in', 'which', 'dow', 'jones', 'says', 'its', 'board', 'is', \"n't\", 'equivalent', 'to', 'be', 'quickly', 'and', 'sole', 'opportunities', 'for', 'a']\n","['disruptions', 'petrochemicals', 'strictly', 'learned', 'the', 'first', 'time', 'soon', 'would', 'argue', 'that', 'the', 'collapsed', 'images', 'on', 'them', 'hospitals', 'to', 'help', 'personnel', 'they', 'kill', 'every', 'general', 'while', 'reaching', 'china', 'to', 'check', 'the', 'country', \"'s\", 'largest', 'opposition', 'services', 'for', 'his', '<unk>', 'marketing', 'department', 'that', 'has', 'fun', 'points', 'out', 'of', 'the', '<unk>', '<unk>', 'atmosphere']\n","['describes', 'that', 'new-issue', 'will', 'make', 'cost-of-living', 'adjustments', 'projected', 'under', 'that', 'roe', 'rice', 'government', '<unk>', 'programs', 'are', '<unk>', 'and', 'disappointing', '<unk>', 'and', 'budget', 'director', 'richard', 'darman', 'is', 'busy', 'one', 'of', 'the', '<unk>', 'leader', '<unk>', '<unk>', 'a', 'government', 'ministers', 'who', \"'d\", 'prefer', 'to', '<unk>', 'and', 'as', 'conservative', 'people', 'just', 'wants', 'to', 'bolster']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:11:25], Epoch [42/50], loss: 3.3346\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:11:41], Epoch [43/50], loss: 3.3277\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:11:57], Epoch [44/50], loss: 3.3243\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:12:13], Epoch [45/50], loss: 3.3196\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:12:29], Epoch [46/50], loss: 3.3118\n","[Generated Sentences]\n","['good', 'luck', 'where', 'she', 'seems', 'to', 'me', 'how', 'necessary', 'to', 'write', 'these', 'cases', 'freeze', 'you', 'ca', \"n't\", 'anything', 'happen', 'in', 'the', 'market', 'on', 'oct.', 'N', 'through', 'N', 'commissioned', 'salesmen', 'would', 'pose', 'a', 'regular', '<unk>', 'for', 'surgery', '<unk>', 'by', 'the', 'global', 'market', 'and', 'weeks', 'themselves', 'as', 'soon', 'as', 'once', 'as', 'full']\n","['vermont-slauson', 'patent', 'enemies', 'are', 'bracing', 'for', 'a', 'severe', 'fax', 'product', 'with', 'mr.', 'lawson', \"'s\", 'volatility', 'interested', 'in', 'antitrust', 'suit', 'to', 'supply', 'such', 'proceedings', 'by', 'adopting', 'a', 'requiring', 'exchange', 'inquiry', 'in', 'ports', 'around', 'the', 'best', 'circumstances', 'from', 'campeau', 'corp.', 'in', 'a', 'transaction', 'that', 'it', 'is', 'discussing', 'an', 'extension', 'of', 'suggestion', 'from']\n","['duke', 'exist', 'brokers', 'was', 'designing', 'and', 'their', 'dream', 'says', 'barry', 'm.', '<unk>', 'associate', 'frank', 'w.', '<unk>', 'as', 'a', 'salesman', 'may', 'prove', 'optimistic', 'about', 'a', 'major', 'lawsuits', 'against', 'the', 'national', 'fabric', 'room', 'market', 'was', 'on', 'the', 'rafale', 'program', 'scheduled', 'for', '<unk>', 'philippine', 'telephone', 'co.', 'of', 'standard', '&', 'poor', \"'s\", 'corp.', 'were']\n","['gamble', 'salesman', 'at', 'smith', 'chairman', 'investor', 'interest', 'in', 'the', 'past', 'N', 'days', 'not', 'only', 'to', 'the', 'level', 'of', '$', 'N', 'or', 'six', 'cents', 'a', 'share', 'compared', 'if', 'the', 'offering', 'situation', 'believe', 'mr.', 'guber', 'and', 'the', 'succeeding', 'everything', 'rose', 'strongly', 'if', 'they', 'fall', 'apart', 'in', 'response', 'to', 'steven', '<unk>', 'for', 'instance']\n","['furs', 'to', 'the', 'chains', 'associated', 'with', 'the', 'company', 'said', 'that', 'its', '$', 'N', 'billion', 'total', '$', 'N', 'billion', 'bank', 'voting', 'funds', 'provided', 'for', 'a', 'year', 'on', 'corporate', 'raiders', 'to', 'N', 'billion', 'francs', 'if', 'campeau', \"'s\", 'convertible', 'capital', 'holding', 'stock', 'would', 'rise', 'about', 'N', 'million', 'shares', 'outstanding', 'as', 'a', 'result', 'of']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:12:45], Epoch [47/50], loss: 3.3057\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:13:01], Epoch [48/50], loss: 3.3019\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:16, 10.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:13:18], Epoch [49/50], loss: 3.2964\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:15, 10.32it/s]"]},{"output_type":"stream","name":"stdout","text":["Time [0:13:34], Epoch [50/50], loss: 3.2953\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"1Ua-_6W2a5Lt"},"source":["# References\n","\n","1. https://github.com/pytorch/examples/tree/master/word_language_model\n","2. https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model"]}]}